You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
INFO:     Started server process [11109]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:9999 (Press CTRL+C to quit)
0 /hy-tmp/
1 /hy-tmp/
0 /hy-tmp/
INFO:     127.0.0.1:49872 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:49886 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:49890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
0 /hy-tmp/
0 /hy-tmp/
0 /hy-tmp/
INFO:     127.0.0.1:48092 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:48104 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:48106 - "POST /v1/chat/completions HTTP/1.1" 200 OK
0 /hy-tmp/
INFO:     127.0.0.1:44538 - "POST /v1/chat/completions HTTP/1.1" 200 OK
0 /hy-tmp/
INFO:     127.0.0.1:44552 - "POST /v1/chat/completions HTTP/1.1" 200 OK
