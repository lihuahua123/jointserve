/usr/local/lib/python3.11/dist-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "cipher": algorithms.TripleDES,
/usr/local/lib/python3.11/dist-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "class": algorithms.TripleDES,
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
INFO:server:Set CUDA_VISIBLE_DEVICES to 0
INFO:server:http://0.0.0.0:30000, ports: PortArgs(tokenizer_port=10000, router_port=10001, detokenizer_port=10002, nccl_port=10003, migrate_port=10004, model_rpc_ports=[10005, 10006, 10007])
INFO:model_rpc:Use sleep forwarding: False
INFO:model_rpc:schedule_heuristic: fcfs-s
INFO:model_runner:Rank 0: load weight begin.
INFO:model_runner:Rank 0: load weight end.
INFO:model_runner:kv one token size: 32 * 128 * 32 * 2 * 2 = 524288 bytes
INFO:model_runner:kv one token size: 32 * 128 * 32 * 2 * 2 = 524288 bytes
INFO:model_runner:self.max_total_num_token,self.max_cpu_num_token 12606,361390
INFO:model_rpc:Rank 0: max_total_num_token=12606, max_prefill_num_token=33768, context_len=33768, 
INFO:model_rpc:server_args: enable_flashinfer=True, attention_reduce_in_fp32=False, disable_radix_cache=False, disable_regex_jump_forward=False, disable_disk_cache=False, 
INFO:     Started server process [7171]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
INFO:model_rpc:Cache flushed successfully!
No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.
INFO:model_rpc:Cache flushed successfully!
INFO:model_rpc:Cache flushed successfully!
INFO:model_rpc:Cache flushed successfully!
INFO:model_rpc:Cache flushed successfully!
INFO:model_rpc:Cache flushed successfully!
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.2000 -> 0.2500
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.2498 -> 0.2998
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 2, #new_token_ratio: 0.2997 -> 0.3497
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 2, #new_token_ratio: 0.3492 -> 0.3992
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.3990 -> 0.4490
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 2, #new_token_ratio: 0.4489 -> 0.4989
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 2, #new_token_ratio: 0.4987 -> 0.5487
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 2, #new_token_ratio: 0.5485 -> 0.5985
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.5984 -> 0.6484
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 2, #new_token_ratio: 0.6480 -> 0.6980
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.6977 -> 0.7477
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.7477 -> 0.7977
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.7975 -> 0.8475
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.8468 -> 0.8968
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.8967 -> 0.9467
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.9466 -> 0.9966
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.9964 -> 1.0000
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.9997 -> 1.0000
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.9999 -> 1.0000
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.9999 -> 1.0000
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.9999 -> 1.0000
