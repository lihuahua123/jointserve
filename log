/usr/local/lib/python3.11/dist-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "cipher": algorithms.TripleDES,
/usr/local/lib/python3.11/dist-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.
  "class": algorithms.TripleDES,
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
INFO:server:Set CUDA_VISIBLE_DEVICES to 0
INFO:server:http://0.0.0.0:30000, ports: PortArgs(tokenizer_port=10000, router_port=10001, detokenizer_port=10002, nccl_port=10003, migrate_port=10004, model_rpc_ports=[10005, 10006, 10007])
INFO:model_rpc:Use sleep forwarding: False
INFO:model_rpc:schedule_heuristic: fcfs-s
INFO:model_runner:Rank 0: load weight begin.
INFO:model_runner:Rank 0: load weight end.
INFO:model_runner:kv one token size: 32 * 128 * 32 * 2 * 2 = 524288 bytes
INFO:model_runner:kv one token size: 32 * 128 * 32 * 2 * 2 = 524288 bytes
INFO:model_runner:self.max_total_num_token,self.max_cpu_num_token 12606,342844
INFO:infer_adapter:load 2 adapters, 2 in total
INFO:model_rpc:Rank 0: max_total_num_token=12606, max_prefill_num_token=33768, context_len=33768, 
INFO:model_rpc:server_args: enable_flashinfer=True, attention_reduce_in_fp32=False, disable_radix_cache=False, disable_regex_jump_forward=False, disable_disk_cache=False, 
/root/jointserve/python/sglang/srt/managers/router/model_rpc.py:724: UserWarning: Warning: available_size=12478, max_total_num_token=12606
KV cache pool leak detected!
  warnings.warn(
INFO:infer_adapter:load 1 adapters, 2 in total
/root/jointserve/python/sglang/srt/managers/router/model_rpc.py:724: UserWarning: Warning: available_size=12542, max_total_num_token=12606
KV cache pool leak detected!
  warnings.warn(
INFO:model_rpc:#running-req: 1, #token: 83, token usage: 0.01, gen throughput (token/s): 7.13, #queue-req: 0
INFO:     Started server process [4284]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.3113 -> 0.3613
model /hy-tmp/ loaded.
lora manager ready.
len(self.forward_queue): 1
12478
have waiting requests can schedule,scheduing req len: 1 0.0006151199340820312 0.0006172657012939453
offload 2 adapters, 0 remains
offload 1 adapters, 1 remains
len(self.forward_queue): 1
12542
have waiting requests can schedule,scheduing req len: 1 0.019337892532348633 0.000812530517578125
offload 2 adapters, 0 remains
offload 1 adapters, 1 remains
len(self.forward_queue): 1
12542
have waiting requests can schedule,scheduing req len: 1 0.037529706954956055 0.00027942657470703125
offload 1 adapters, 0 remains
len(self.forward_queue): 1
12542
have waiting requests can schedule,scheduing req len: 1 0.055127620697021484 0.0018839836120605469
offload 1 adapters, 0 remains
len(self.forward_queue): 1
12542
have waiting requests can schedule,scheduing req len: 1 0.6900777816772461 0.0012121200561523438
len(self.forward_queue): 1
12215.924999999997
have waiting requests can schedule,scheduing req len: 1 0.6907608509063721 0.0006837844848632812
len(self.forward_queue): 4
11944.587199999993
have waiting requests can schedule,scheduing req len: 4 0.6985111236572266 0.00046634674072265625
len(self.forward_queue): 2
10860.39199999998
have waiting requests can schedule,scheduing req len: 2 0.6989521980285645 0.00044155120849609375
len(self.forward_queue): 2
10558.333599999976
have waiting requests can schedule,scheduing req len: 2 0.7114903926849365 0.0007736682891845703
len(self.forward_queue): 1
10037.63999999997
have waiting requests can schedule,scheduing req len: 1 0.7117528915405273 0.00026297569274902344
len(self.forward_queue): 3
8971.508799999965
have waiting requests can schedule,scheduing req len: 3 0.7124507427215576 0.0006988048553466797
len(self.forward_queue): 6
5575.464799999949
have waiting requests can schedule,scheduing req len: 5 0.7138364315032959 0.0013866424560546875
len(self.forward_queue): 2
2681.6255999999357
have waiting requests can schedule,scheduing req len: 2 0.714350700378418 0.0005147457122802734
len(self.forward_queue): 3
1213.8863999999248
have waiting requests can schedule,scheduing req len: 1 0.7147865295410156 0.0004360675811767578
len(self.forward_queue): 2
884.2423999999182
have waiting requests can schedule,scheduing req len: 1 0.7151050567626953 0.0003190040588378906
len(self.forward_queue): 3
698.7535999999145
have waiting requests can schedule,scheduing req len: 1 0.715531587600708 0.0004267692565917969
len(self.forward_queue): 3
556.3063999999122
have waiting requests but can't schedule 0.7163100242614746 0.0008847713470458984
len(self.forward_queue): 6
440.516799999913
have waiting requests but can't schedule 0.733100414276123 0.0006084442138671875
len(self.forward_queue): 6
324.41999999991276
have waiting requests but can't schedule 0.7480061054229736 0.0004918575286865234
len(self.forward_queue): 6
208.01599999991413
have waiting requests but can't schedule 0.7631406784057617 0.000492095947265625
len(self.forward_queue): 6
91.3047999999144
have waiting requests but can't schedule 0.7779734134674072 0.0005741119384765625
len(self.forward_queue): 6
-25.713600000086444
have waiting requests but can't schedule 0.7929928302764893 0.00047016143798828125
len(self.forward_queue): 6
-143.03920000008566
have waiting requests but can't schedule 0.807772159576416 0.0005748271942138672
offload 0 adapters, 1 remains
len(self.forward_queue): 6
-196.6720000000846
have waiting requests but can't schedule 0.8225293159484863 0.0005540847778320312
len(self.forward_queue): 6
-309.293600000085
have waiting requests but can't schedule 0.8372271060943604 0.0005068778991699219
len(self.forward_queue): 6
-422.20960000008563
have waiting requests but can't schedule 0.851776123046875 0.00048542022705078125
len(self.forward_queue): 6
-535.4200000000851
have waiting requests but can't schedule 0.8664684295654297 0.0004432201385498047
len(self.forward_queue): 6
-648.9248000000835
have waiting requests but can't schedule 0.8824679851531982 0.0007138252258300781
len(self.forward_queue): 6
-762.7240000000838
have waiting requests but can't schedule 0.8979549407958984 0.0004239082336425781
len(self.forward_queue): 6
-876.8176000000844
have waiting requests but can't schedule 0.9136087894439697 0.0008876323699951172
len(self.forward_queue): 6
-991.205600000083
have waiting requests but can't schedule 0.9295473098754883 0.0004444122314453125
len(self.forward_queue): 6
-1105.8880000000822
have waiting requests but can't schedule 0.9444160461425781 0.00046181678771972656
len(self.forward_queue): 6
-1220.864800000083
have waiting requests but can't schedule 0.9596419334411621 0.0003955364227294922
len(self.forward_queue): 6
-1336.1360000000823
have waiting requests but can't schedule 0.9749960899353027 0.0009629726409912109
len(self.forward_queue): 6
-1451.7016000000813
have waiting requests but can't schedule 0.9926497936248779 0.0007894039154052734
offload 0 adapters, 1 remains
len(self.forward_queue): 6
-1170.5948000000812
have waiting requests but can't schedule 1.0006184577941895 0.0005667209625244141
len(self.forward_queue): 6
-1225.867200000081
have waiting requests but can't schedule 1.0081911087036133 0.0004150867462158203
len(self.forward_queue): 6
-1336.6232000000796
have waiting requests but can't schedule 1.024479866027832 0.0008077621459960938
len(self.forward_queue): 6
-1447.6608000000797
have waiting requests but can't schedule 1.0410311222076416 0.0011043548583984375
len(self.forward_queue): 6
-1558.9800000000791
have waiting requests but can't schedule 1.0595159530639648 0.0006771087646484375
len(self.forward_queue): 6
-1670.5808000000784
have waiting requests but can't schedule 1.0746440887451172 0.0006227493286132812
len(self.forward_queue): 6
-1782.463200000077
have waiting requests but can't schedule 1.0916590690612793 0.0006084442138671875
len(self.forward_queue): 6
-1894.6272000000772
have waiting requests but can't schedule 1.1089956760406494 0.0006346702575683594
len(self.forward_queue): 6
-2007.0728000000763
have waiting requests but can't schedule 1.126673698425293 0.0005967617034912109
len(self.forward_queue): 6
-2119.8000000000757
have waiting requests but can't schedule 1.1419682502746582 0.0006508827209472656
len(self.forward_queue): 6
-2232.8088000000744
have waiting requests but can't schedule 1.1586613655090332 0.0007655620574951172
offload 0 adapters, 1 remains
len(self.forward_queue): 6
-2053.418800000074
have waiting requests but can't schedule 1.1678729057312012 0.0005457401275634766
len(self.forward_queue): 6
-2107.3728000000738
have waiting requests but can't schedule 1.1773185729980469 0.0006024837493896484
offload 0 adapters, 1 remains
len(self.forward_queue): 6
-1853.429800000073
have waiting requests but can't schedule 1.1915557384490967 0.0006539821624755859
len(self.forward_queue): 6
-1879.1176000000728
have waiting requests but can't schedule 1.1974022388458252 0.0005171298980712891
len(self.forward_queue): 6
-1982.0288000000724
have waiting requests but can't schedule 1.2128510475158691 0.0005867481231689453
len(self.forward_queue): 6
-2085.1960000000718
have waiting requests but can't schedule 1.2284905910491943 0.0008094310760498047
len(self.forward_queue): 6
-2188.6192000000706
have waiting requests but can't schedule 1.2452902793884277 0.00046062469482421875
offload 0 adapters, 1 remains
len(self.forward_queue): 6
-1646.51500000007
have waiting requests but can't schedule 1.2497081756591797 0.0008313655853271484
len(self.forward_queue): 6
-1720.1848000000696
have waiting requests but can't schedule 1.2625460624694824 0.0006272792816162109
len(self.forward_queue): 6
-1818.6240000000694
have waiting requests but can't schedule 1.2772400379180908 0.0005118846893310547
len(self.forward_queue): 6
-1917.3064000000682
have waiting requests but can't schedule 1.2921655178070068 0.0008041858673095703
len(self.forward_queue): 6
-2016.2320000000673
have waiting requests but can't schedule 1.3075504302978516 0.0007636547088623047
len(self.forward_queue): 7
-2109.199800000065
have waiting requests but can't schedule 1.3252372741699219 0.0011110305786132812
offload 0 adapters, 1 remains
len(self.forward_queue): 7
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.3558 -> 0.4058
INFO:model_rpc:GPU 0: decode out of memory happened, #retracted_reqs: 1, #new_token_ratio: 0.4036 -> 0.4536
-1519.1120000000642
have waiting requests but can't schedule 1.3406705856323242 0.0012934207916259766
len(self.forward_queue): 7
-1529.3315000000644
have waiting requests but can't schedule 1.3452441692352295 0.0009052753448486328
len(self.forward_queue): 7
-1611.2099000000635
have waiting requests but can't schedule 1.3609378337860107 0.0007278919219970703
len(self.forward_queue): 7
-1693.3059000000621
have waiting requests but can't schedule 1.3767926692962646 0.0004241466522216797
len(self.forward_queue): 7
-1775.619500000062
have waiting requests but can't schedule 1.3909361362457275 0.0004470348358154297
len(self.forward_queue): 7
-1858.1507000000615
have waiting requests but can't schedule 1.406048059463501 0.0007183551788330078
len(self.forward_queue): 7
-1940.89950000006
have waiting requests but can't schedule 1.4208838939666748 0.0005190372467041016
len(self.forward_queue): 8
-1874.523200000057
have waiting requests but can't schedule 1.4363913536071777 0.0007700920104980469
len(self.forward_queue): 8
-1946.2800000000557
have waiting requests but can't schedule 1.4516232013702393 0.0007686614990234375
len(self.forward_queue): 8
-2018.2416000000553
have waiting requests but can't schedule 1.465461254119873 0.0004525184631347656
offload 0 adapters, 1 remains
len(self.forward_queue): 9
-480.56100000004517
have waiting requests but can't schedule 1.4811244010925293 0.0004918575286865234
len(self.forward_queue): 9
-538.4618000000441
have waiting requests but can't schedule 1.4974184036254883 0.001224517822265625
len(self.forward_queue): 9
-596.5418000000436
have waiting requests but can't schedule 1.5132839679718018 0.0004825592041015625
offload 0 adapters, 1 remains
len(self.forward_queue): 9
-199.64900000004286
have waiting requests but can't schedule 1.5225045680999756 0.0012886524200439453
len(self.forward_queue): 9
-226.60540000004266
have waiting requests but can't schedule 1.531259298324585 0.0005967617034912109
len(self.forward_queue): 9
-280.64300000004187
have waiting requests but can't schedule 1.5442008972167969 0.0005910396575927734
len(self.forward_queue): 9
-334.84700000004113
have waiting requests but can't schedule 1.55706787109375 0.0007054805755615234
len(self.forward_queue): 9
-389.21740000004047
have waiting requests but can't schedule 1.5705795288085938 0.00080108642578125
len(self.forward_queue): 9
-443.7542000000392
have waiting requests but can't schedule 1.5845859050750732 0.0008859634399414062
len(self.forward_queue): 9
-498.4574000000382
have waiting requests but can't schedule 1.5986158847808838 0.0006792545318603516
offload 0 adapters, 1 remains
len(self.forward_queue): 9
-89.32700000003751
have waiting requests but can't schedule 1.6119709014892578 0.0004680156707763672
len(self.forward_queue): 9
-139.92700000003674
have waiting requests but can't schedule 1.6242787837982178 0.0005803108215332031
len(self.forward_queue): 9
-190.68060000003584
have waiting requests but can't schedule 1.6370172500610352 0.000476837158203125
len(self.forward_queue): 9
-241.5878000000348
have waiting requests but can't schedule 1.6494193077087402 0.0005865097045898438
len(self.forward_queue): 9
-292.6486000000343
have waiting requests but can't schedule 1.6628761291503906 0.0008368492126464844
len(self.forward_queue): 9
-343.8630000000335
have waiting requests but can't schedule 1.677687644958496 0.0008535385131835938
len(self.forward_queue): 9
-395.2310000000325
have waiting requests but can't schedule 1.6938960552215576 0.001291513442993164
offload 0 adapters, 1 remains
len(self.forward_queue): 9
84.69599999996831
have waiting requests but can't schedule 1.7070627212524414 0.0006830692291259766
len(self.forward_queue): 9
78.80669999996871
have waiting requests but can't schedule 1.7099707126617432 0.0012660026550292969
len(self.forward_queue): 9
31.613099999969336
have waiting requests but can't schedule 1.7253129482269287 0.0007340908050537109
len(self.forward_queue): 9
-15.721300000029714
have waiting requests but can't schedule 1.7391672134399414 0.0007579326629638672
len(self.forward_queue): 9
-63.19650000002889
have waiting requests but can't schedule 1.7543036937713623 0.0005402565002441406
len(self.forward_queue): 9
-110.81250000002808
have waiting requests but can't schedule 1.7673630714416504 0.0009248256683349609
len(self.forward_queue): 9
-158.56930000002706
have waiting requests but can't schedule 1.7811639308929443 0.0007042884826660156
len(self.forward_queue): 9
-206.46690000002627
have waiting requests but can't schedule 1.7942743301391602 0.0008885860443115234
len(self.forward_queue): 9
-254.50530000002527
have waiting requests but can't schedule 1.807882308959961 0.0005812644958496094
len(self.forward_queue): 9
-302.6845000000243
have waiting requests but can't schedule 1.8208985328674316 0.0004820823669433594
len(self.forward_queue): 9
-351.0045000000233
have waiting requests but can't schedule 1.8342275619506836 0.0006165504455566406
len(self.forward_queue): 9
-399.46530000002235
have waiting requests but can't schedule 1.8474442958831787 0.0011131763458251953
len(self.forward_queue): 9
-448.0669000000214
have waiting requests but can't schedule 1.8614249229431152 0.0004761219024658203
offload 0 adapters, 1 remains
len(self.forward_queue): 9
489.8479999999786
have waiting requests but can't schedule 1.8634910583496094 0.0004627704620361328
len(self.forward_queue): 9
451.17299999997954
have waiting requests but can't schedule 1.874131679534912 0.0004742145538330078
len(self.forward_queue): 9
406.8529999999805
have waiting requests but can't schedule 1.8867974281311035 0.0006861686706542969
len(self.forward_queue): 9
362.4049999999813
have waiting requests but can't schedule 1.899604082107544 0.0007216930389404297
len(self.forward_queue): 9
317.8289999999822
have waiting requests but can't schedule 1.9135444164276123 0.0006992816925048828
offload 0 adapters, 1 remains
len(self.forward_queue): 9
1611.0799999999826
have waiting requests can schedule,scheduing req len: 2 1.918764591217041 0.0007369518280029297
len(self.forward_queue): 7
746.5775999999662
have waiting requests can schedule,scheduing req len: 1 1.9194002151489258 0.0006358623504638672
len(self.forward_queue): 6
578.6339999999618
have waiting requests can schedule,scheduing req len: 1 1.9200286865234375 0.0006291866302490234
len(self.forward_queue): 5
376.99479999995583
have waiting requests but can't schedule 1.920576810836792 0.00054931640625
len(self.forward_queue): 5
341.45979999995643
have waiting requests but can't schedule 1.9294652938842773 0.0005340576171875
len(self.forward_queue): 5
284.4685999999574
have waiting requests but can't schedule 1.9437801837921143 0.0012543201446533203
len(self.forward_queue): 5
227.31099999995877
have waiting requests but can't schedule 1.9606432914733887 0.0005168914794921875
len(self.forward_queue): 5
169.98699999996006
have waiting requests but can't schedule 1.9743807315826416 0.0005202293395996094
len(self.forward_queue): 5
112.49659999996106
have waiting requests but can't schedule 1.9875853061676025 0.0005297660827636719
offload 0 adapters, 1 remains
len(self.forward_queue): 5
755.2695999999619
have waiting requests can schedule,scheduing req len: 1 1.998915672302246 0.0003764629364013672
len(self.forward_queue): 4
534.8564999999578
have waiting requests but can't schedule 1.999220609664917 0.0003058910369873047
len(self.forward_queue): 4
520.4848999999581
have waiting requests but can't schedule 2.0027108192443848 0.0003314018249511719
len(self.forward_queue): 4
462.8944999999594
have waiting requests but can't schedule 2.017300605773926 0.0006024837493896484
offload 0 adapters, 1 remains
len(self.forward_queue): 4
1121.3663999999605
have waiting requests can schedule,scheduing req len: 1 2.0289087295532227 0.00033092498779296875
len(self.forward_queue): 3
732.7583999999476
have waiting requests can schedule,scheduing req len: 1 2.029226779937744 0.00031876564025878906
len(self.forward_queue): 2
556.6175999999418
have waiting requests can schedule,scheduing req len: 1 2.0295541286468506 0.00032782554626464844
len(self.forward_queue): 1
327.2543999999343
have waiting requests but can't schedule You pressed Ctrl+C! Shutting down all remote servers...
INFO:     Shutting down
2.0300803184509277 0.0005271434783935547
len(self.forward_queue): 1
319.0582999999342
have waiting requests but can't schedule 2.0324056148529053 0.000537872314453125
len(self.forward_queue): 1
253.38149999993516
have waiting requests but can't schedule 2.047126293182373 0.0002148151397705078
len(self.forward_queue): 1
187.51269999993679
have waiting requests but can't schedule 2.0612378120422363 0.0005450248718261719
len(self.forward_queue): 1
121.45189999993818
have waiting requests but can't schedule 2.076615571975708 0.0005197525024414062
len(self.forward_queue): 1
55.19909999993979
have waiting requests but can't schedule 2.09016489982605 0.0002663135528564453
offload 0 adapters, 1 remains
len(self.forward_queue): 1
746.6935999999403
have waiting requests but can't schedule 2.0990967750549316 0.00020003318786621094
len(self.forward_queue): 1
723.4993999999408
have waiting requests but can't schedule 2.10455322265625 0.0004990100860595703
len(self.forward_queue): 1
661.5249999999421
have waiting requests but can't schedule 2.1180410385131836 0.0002655982971191406
len(self.forward_queue): 1
599.3713999999434
have waiting requests but can't schedule 2.1314775943756104 0.0002181529998779297
len(self.forward_queue): 1
537.0385999999448
have waiting requests but can't schedule 2.1451845169067383 0.0003521442413330078
len(self.forward_queue): 1
474.52659999994603
have waiting requests but can't schedule 2.160223960876465 0.0004639625549316406
offload 0 adapters, 1 remains
len(self.forward_queue): 1
1201.5249999999469
have waiting requests can schedule,scheduing req len: 1 2.1719119548797607 0.00029015541076660156
offload 0 adapters, 1 remains
offload 0 adapters, 1 remains
offload 0 adapters, 1 remains
offload 0 adapters, 1 remains
offload 0 adapters, 1 remains
offload 0 adapters, 1 remains
offload 0 adapters, 1 remains
offload 0 adapters, 1 remains
offload 0 adapters, 1 remains
offload 0 adapters, 1 remains
offload 0 adapters, 1 remains
offload 0 adapters, 1 remains
offload 0 adapters, 1 remains
offload 1 adapters, 0 remains
You pressed Ctrl+C! Shutting down all remote servers...
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [4284]
Server is on port 30000 on host 0.0.0.0 on pid 4441
You pressed Ctrl+C! Shutting down all remote servers...
You pressed Ctrl+C! Shutting down all remote servers...
Loading runtimes at ['http://0.0.0.0:30000/generate']
You pressed Ctrl+C! Shutting down all remote servers...
You pressed Ctrl+C! Shutting down all remote servers...
